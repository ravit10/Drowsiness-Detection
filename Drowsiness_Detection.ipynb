{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import imutils\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FileVideoStream\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from threading import Thread\n",
    "import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running the file from command line, then can pass arguments using the below code\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-p', '--shape-predictor', required = True, help = 'path to facial landmark detector')\n",
    "# ap.add_arguemnt('-v', '--video', type = str, default = '', help = 'path to input video file')\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "SHAPE_PREDICTOR_PATH = 'drowsiness-detection/shape_predictor_68_face_landmarks.dat'\n",
    "ALARM_AUDIO_PATH = 'drowsiness-detection/alarm.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of eye landmark distances to determine if a person is blinking\n",
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    vertical_A = dist.euclidean(eye[1], eye[5])\n",
    "    vertical_B = dist.euclidean(eye[2], eye[4])\n",
    "    \n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    horizontal_C = dist.euclidean(eye[0], eye[3])\n",
    "    \n",
    "    # compute the eye aspect ratio\n",
    "    ear = (vertical_A + vertical_B) / (2.0 * horizontal_C)\n",
    "    \n",
    "    # return the eye aspect ratio\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold constants to detect a drowsiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant for the eye aspect ratio to indicate drowsiness \n",
    "EYE_AR_THRESH = 0.3\n",
    "\n",
    "# Constant for the number of consecutive frames the eye (closed) must be below the threshold\n",
    "EYE_AR_CONSEC_FRAMES = 48\n",
    "\n",
    "# Initialize the frame counter\n",
    "FRAME_COUNTER = 0\n",
    "\n",
    "# Boolean to indicate if the alarm is going off\n",
    "IS_ALARM_ON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a bounding predicted by dlib and convert it\n",
    "# to the format (x, y, w, h) as normally handled in OpenCV\n",
    "def rect_to_bb(rect):\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    \n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dlib face landmark detector will return a shape object \n",
    "# containing the 68 (x, y)-coordinates of the facial landmark regions.\n",
    "# This fucntion converts the above object to a NumPy array.\n",
    "def shape_to_np(shape, dtype = 'int'):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype = dtype)\n",
    "    \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sound_alarm(path):\n",
    "    # play an alarm sound\n",
    "    playsound.playsound(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Landmarks indices based on the 68-point facial landmark detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps the indexes of the facial\n",
    "# landmarks to specific face regions\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"mouth\", (48, 68)),\n",
    "    (\"right_eyebrow\", (17, 22)),\n",
    "    (\"left_eyebrow\", (22, 27)),\n",
    "    (\"right_eye\", (36, 42)),\n",
    "    (\"left_eye\", (42, 48)),\n",
    "    (\"nose\", (27, 35)),\n",
    "    (\"jaw\", (0, 17))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dlib's face detector (HOG-based)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# create the facial landmark predictor\n",
    "predictor = dlib.shape_predictor(SHAPE_PREDICTOR_PATH)\n",
    "\n",
    "# grab the indexes of the facial landmarks for the left and\n",
    "# right eye, respectively\n",
    "(leStart, leEnd) = FACIAL_LANDMARKS_IDXS['left_eye']\n",
    "(reStart, reEnd) = FACIAL_LANDMARKS_IDXS['right_eye']\n",
    "\n",
    "# start the video stream thread\n",
    "# Streaming from a video file\n",
    "# vs = FileVideoStream(VIDEO_PATH).start()\n",
    "# fileStream = True\n",
    "\n",
    "# Streaming from a web-cam\n",
    "vs = VideoStream(src = 0).start()\n",
    "fileStream = False\n",
    "\n",
    "time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Face, Eyes and Blinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5dd03d0a1609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# it, and convert it to grayscale channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m450\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imutils\\convenience.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, width, height, inter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# grab the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# if both the width and height are None, then return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# loop over frames from the video stream\n",
    "while True:\n",
    "    # if this is a file video stream, then we need to check if\n",
    "    # there any more frames left in the buffer to process\n",
    "    if fileStream and not vs.more():\n",
    "        break\n",
    "        \n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale channels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width = 450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Detect faces in the grayscale image\n",
    "    # Note: The 0 in the second argument indicates that we will not upsample the image. \n",
    "    # The benefit of increasing the resolution of the input image prior to face detection is that \n",
    "    # it may allow us to detect more faces in the image — the downside is that the larger the input image, \n",
    "    # the more computaitonally expensive the detection process is.\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # loop over the detected faces\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape_np = shape_to_np(shape)\n",
    "        \n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape_np[leStart:leEnd]\n",
    "        rightEye = shape_np[reStart:reEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        \n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        avgEAR = (leftEAR + rightEAR) / 2.0\n",
    "        \n",
    "        # compute the convex hull for the left and right eye, then\n",
    "        # visualize each of the eyes\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if avgEAR < EYE_AR_THRESH:\n",
    "            FRAME_COUNTER += 1\n",
    "            \n",
    "            # if the eyes were closed for a sufficient number of\n",
    "            # then sound the alarm\n",
    "            if FRAME_COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                # if the alarm is not on, turn it on\n",
    "                if not IS_ALARM_ON:\n",
    "                    IS_ALARM_ON = True\n",
    "                    \n",
    "                    # check to see if an alarm file was supplied,\n",
    "                    # and if so, start a thread to have the alarm\n",
    "                    # sound played in the background\n",
    "                    if ALARM_AUDIO_PATH != '':\n",
    "                        t = Thread(target = sound_alarm, args = (ALARM_AUDIO_PATH,))\n",
    "                        t.deamon = True\n",
    "                        t.start()\n",
    "                        \n",
    "                cv2.putText(frame, 'DROWSINESS ALERT!!!', (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        else:\n",
    "            FRAME_COUNTER = 0\n",
    "            IS_ALARM_ON = False\n",
    "            \n",
    "        # draw the computed eye aspect ratio for the frame\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(avgEAR), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "    # show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
